{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a108838",
   "metadata": {},
   "source": [
    "# An√°lisis de Par√°metros del Algoritmo Ant Colony System (ACS)\n",
    "\n",
    "Este notebook proporciona un an√°lisis completo de los par√°metros del algoritmo ACS para el problema del viajante de comercio (TSP Berlin52). \n",
    "\n",
    "## Informaci√≥n del Problema:\n",
    "- **Instancia**: Berlin52 (52 ciudades)\n",
    "- **√ìptimo conocido**: 7544.3659 (verificado por m√∫ltiples fuentes)\n",
    "- **Objetivo**: Encontrar los mejores par√°metros para el algoritmo ACS\n",
    "\n",
    "## Objetivos del An√°lisis:\n",
    "1. Validar la correctitud de los tests de par√°metros\n",
    "2. Realizar an√°lisis estad√≠stico robusto de los resultados\n",
    "3. Identificar los mejores par√°metros para el algoritmo\n",
    "4. Proporcionar visualizaciones comprensivas del rendimiento\n",
    "\n",
    "## Metodolog√≠a:\n",
    "- M√∫ltiples ejecuciones independientes para cada configuraci√≥n de par√°metros\n",
    "- An√°lisis estad√≠stico con intervalos de confianza\n",
    "- Visualizaciones para identificar patrones y tendencias\n",
    "- Recomendaciones basadas en evidencia estad√≠stica\n",
    "- C√°lculo de gap porcentual basado en el √≥ptimo conocido (7544.3659)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe34c59",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a13eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as b√°sicas para an√°lisis de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Librer√≠as para an√°lisis estad√≠stico\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Configuraci√≥n de pandas para mostrar m√°s columnas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
    "print(\"üìä Configuraci√≥n de visualizaci√≥n establecida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab41569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n espec√≠fica del problema\n",
    "BERLIN52_OPTIMAL_COST = 7544.3659  # Valor √≥ptimo verificado para Berlin52\n",
    "TSP_FILE = \"./berlin52.tsp\"\n",
    "\n",
    "print(f\"üéØ Problema: Berlin52 TSP\")\n",
    "print(f\"üìä Valor √≥ptimo conocido: {BERLIN52_OPTIMAL_COST}\")\n",
    "print(f\"üìÅ Archivo TSP: {TSP_FILE}\")\n",
    "print(f\"üîç Todos los an√°lisis de gap se basar√°n en este valor √≥ptimo verificado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3e98a",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c8f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para cargar datos de an√°lisis de par√°metros\n",
    "def load_parameter_analysis_data(file_path=None, recalculate_gap=True):\n",
    "    \"\"\"\n",
    "    Carga los datos del an√°lisis de par√°metros desde un archivo JSON.\n",
    "    Si no se especifica archivo, busca el m√°s reciente.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Ruta al archivo JSON (opcional)\n",
    "        recalculate_gap: Si True, recalcula el gap con el valor √≥ptimo correcto\n",
    "    \"\"\"\n",
    "    if file_path is None:\n",
    "        # Buscar el archivo m√°s reciente\n",
    "        current_dir = Path('.')\n",
    "        json_files = list(current_dir.glob('*parameter_analysis_*.json')) + \\\n",
    "                    list(current_dir.glob('*statistical_tsp_results_*.json'))\n",
    "        if not json_files:\n",
    "            print(\"‚ùå No se encontraron archivos de an√°lisis.\")\n",
    "            print(\"üí° Ejecuta primero uno de estos scripts:\")\n",
    "            print(\"   - parameter_analysis_test.py\")\n",
    "            print(\"   - improved_statistical_test.py\")\n",
    "            return None\n",
    "        file_path = max(json_files, key=lambda x: x.stat().st_mtime)\n",
    "        print(f\"üìÇ Cargando archivo m√°s reciente: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Recalcular gap percentage con el valor √≥ptimo correcto si es necesario\n",
    "        if recalculate_gap:\n",
    "            df['gap_percentage'] = ((df['final_cost'] - BERLIN52_OPTIMAL_COST) / BERLIN52_OPTIMAL_COST) * 100\n",
    "            print(f\"üîÑ Gap percentage recalculado con √≥ptimo correcto: {BERLIN52_OPTIMAL_COST}\")\n",
    "        \n",
    "        print(f\"‚úÖ Datos cargados: {len(df)} experimentos\")\n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Archivo no encontrado: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al cargar datos: {e}\")\n",
    "        return None\n",
    "\n",
    "# Cargar datos\n",
    "df = load_parameter_analysis_data()\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"\\nüìä Informaci√≥n del dataset:\")\n",
    "    print(f\"   - Forma: {df.shape}\")\n",
    "    print(f\"   - Par√°metros analizados: {df[['colony_size', 'alpha', 'beta', 'q0']].nunique().to_dict()}\")\n",
    "    print(f\"   - Rango de costos: {df['final_cost'].min():.2f} - {df['final_cost'].max():.2f}\")\n",
    "    print(f\"   - Mejor costo vs √≥ptimo: {df['final_cost'].min():.2f} vs {BERLIN52_OPTIMAL_COST}\")\n",
    "    print(f\"   - Rango de gaps: {df['gap_percentage'].min():.2f}% - {df['gap_percentage'].max():.2f}%\")\n",
    "    \n",
    "    # Verificar si alguna corrida alcanz√≥ el √≥ptimo\n",
    "    optimal_reached = (df['final_cost'] <= BERLIN52_OPTIMAL_COST + 0.01).sum()\n",
    "    if optimal_reached > 0:\n",
    "        print(f\"   üéØ Corridas que alcanzaron el √≥ptimo (¬±0.01): {optimal_reached}\")\n",
    "    \n",
    "    # Mostrar primeras filas\n",
    "    print(f\"\\nüìã Primeras filas del dataset:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No se pudieron cargar los datos.\")\n",
    "    print(\"Ejecuta uno de estos comandos para generar datos:\")\n",
    "    print(\"python parameter_analysis_test.py\")\n",
    "    print(\"python improved_statistical_test.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e0ccc",
   "metadata": {},
   "source": [
    "## 3. Validate Test Correctness\n",
    "\n",
    "Antes de proceder con el an√°lisis, validemos que la metodolog√≠a de testing es correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a274914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_test_methodology(df):\n",
    "    \"\"\"\n",
    "    Valida la correctitud de la metodolog√≠a de testing\n",
    "    \"\"\"\n",
    "    print(\"üîç VALIDACI√ìN DE LA METODOLOG√çA DE TESTING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Verificar m√∫ltiples ejecuciones\n",
    "    runs_per_config = df.groupby(['colony_size', 'alpha', 'beta', 'q0']).size()\n",
    "    min_runs = runs_per_config.min()\n",
    "    max_runs = runs_per_config.max()\n",
    "    avg_runs = runs_per_config.mean()\n",
    "    \n",
    "    print(f\"1. üìä M√öLTIPLES EJECUCIONES:\")\n",
    "    print(f\"   - M√≠nimo de corridas: {min_runs}\")\n",
    "    print(f\"   - M√°ximo de corridas: {max_runs}\")\n",
    "    print(f\"   - Promedio de corridas: {avg_runs:.1f}\")\n",
    "    \n",
    "    if min_runs >= 3:\n",
    "        print(\"   ‚úÖ BIEN: M√∫ltiples ejecuciones por configuraci√≥n\")\n",
    "    else:\n",
    "        print(\"   ‚ùå PROBLEMA: Pocas ejecuciones, resultados no confiables\")\n",
    "    \n",
    "    # 2. Verificar variabilidad en los resultados\n",
    "    print(f\"\\n2. üìà VARIABILIDAD DE RESULTADOS:\")\n",
    "    variability_stats = df.groupby(['colony_size', 'alpha', 'beta', 'q0'])['final_cost'].agg(['std', 'mean'])\n",
    "    variability_stats['cv'] = variability_stats['std'] / variability_stats['mean']\n",
    "    avg_cv = variability_stats['cv'].mean()\n",
    "    \n",
    "    print(f\"   - Coeficiente de variaci√≥n promedio: {avg_cv:.3f}\")\n",
    "    \n",
    "    if avg_cv > 0.001:\n",
    "        print(\"   ‚úÖ BIEN: Hay variabilidad suficiente en los resultados\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  ADVERTENCIA: Poca variabilidad, posible problema de semillas\")\n",
    "    \n",
    "    # 3. Verificar distribuci√≥n de par√°metros\n",
    "    print(f\"\\n3. ‚öñÔ∏è  DISTRIBUCI√ìN DE PAR√ÅMETROS:\")\n",
    "    for param in ['colony_size', 'alpha', 'beta', 'q0']:\n",
    "        unique_values = df[param].nunique()\n",
    "        print(f\"   - {param}: {unique_values} valores √∫nicos\")\n",
    "    \n",
    "    print(\"   ‚úÖ BIEN: Se probaron m√∫ltiples valores de cada par√°metro\")\n",
    "    \n",
    "    # 4. Verificar semillas diferentes\n",
    "    print(f\"\\n4. üé≤ VERIFICACI√ìN DE SEMILLAS:\")\n",
    "    unique_seeds = df['seed'].nunique()\n",
    "    total_runs = len(df)\n",
    "    \n",
    "    print(f\"   - Semillas √∫nicas: {unique_seeds}\")\n",
    "    print(f\"   - Total de ejecuciones: {total_runs}\")\n",
    "    \n",
    "    if unique_seeds == total_runs:\n",
    "        print(\"   ‚úÖ EXCELENTE: Cada ejecuci√≥n usa una semilla diferente\")\n",
    "    elif unique_seeds >= total_runs * 0.8:\n",
    "        print(\"   ‚úÖ BIEN: La mayor√≠a de ejecuciones usan semillas diferentes\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  ADVERTENCIA: Muchas ejecuciones comparten semillas\")\n",
    "    \n",
    "    # 5. Verificar rango de resultados\n",
    "    print(f\"\\n5. üéØ RANGO DE RESULTADOS:\")\n",
    "    cost_range = df['final_cost'].max() - df['final_cost'].min()\n",
    "    gap_range = df['gap_percentage'].max() - df['gap_percentage'].min()\n",
    "    \n",
    "    print(f\"   - Rango de costos: {cost_range:.2f}\")\n",
    "    print(f\"   - Rango de gaps: {gap_range:.2f}%\")\n",
    "    \n",
    "    if gap_range > 5:\n",
    "        print(\"   ‚úÖ BIEN: Amplio rango de resultados permite discriminar par√°metros\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  ADVERTENCIA: Rango estrecho, dif√≠cil discriminar par√°metros\")\n",
    "    \n",
    "    return {\n",
    "        'multiple_runs': min_runs >= 3,\n",
    "        'sufficient_variability': avg_cv > 0.001,\n",
    "        'unique_seeds': unique_seeds >= total_runs * 0.8,\n",
    "        'good_range': gap_range > 5\n",
    "    }\n",
    "\n",
    "# Ejecutar validaci√≥n si tenemos datos\n",
    "if df is not None:\n",
    "    validation_results = validate_test_methodology(df)\n",
    "    \n",
    "    print(f\"\\nüèÜ RESUMEN DE VALIDACI√ìN:\")\n",
    "    all_good = all(validation_results.values())\n",
    "    if all_good:\n",
    "        print(\"‚úÖ METODOLOG√çA EXCELENTE: Todos los criterios se cumplen\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  METODOLOG√çA MEJORABLE: Algunos criterios no se cumplen completamente\")\n",
    "        print(\"üí° Considera ejecutar m√°s corridas para mejorar la confiabilidad\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No hay datos para validar. Ejecuta primero el an√°lisis de par√°metros.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d800f681",
   "metadata": {},
   "source": [
    "## 4. Statistical Analysis of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a28c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics_by_parameter(df, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Calcula estad√≠sticas detalladas agrupadas por cada par√°metro\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    # Configurar el nivel de confianza\n",
    "    alpha = 1 - confidence_level\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for param in ['colony_size', 'alpha', 'beta', 'q0']:\n",
    "        print(f\"\\nüìä AN√ÅLISIS ESTAD√çSTICO: {param.upper()}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Agrupar por el par√°metro\n",
    "        grouped = df.groupby(param)\n",
    "        \n",
    "        stats_list = []\n",
    "        for value, group in grouped:\n",
    "            n = len(group)\n",
    "            mean_cost = group['final_cost'].mean()\n",
    "            std_cost = group['final_cost'].std()\n",
    "            \n",
    "            # Intervalo de confianza para la media\n",
    "            sem = std_cost / np.sqrt(n)  # Error est√°ndar de la media\n",
    "            t_value = stats.t.ppf(1 - alpha/2, df=n-1)\n",
    "            ci_lower = mean_cost - t_value * sem\n",
    "            ci_upper = mean_cost + t_value * sem\n",
    "            \n",
    "            # Gap percentage\n",
    "            mean_gap = group['gap_percentage'].mean()\n",
    "            std_gap = group['gap_percentage'].std()\n",
    "            \n",
    "            # Tiempo de ejecuci√≥n\n",
    "            mean_time = group['execution_time'].mean()\n",
    "            \n",
    "            stats_dict = {\n",
    "                param: value,\n",
    "                'n_runs': n,\n",
    "                'mean_cost': mean_cost,\n",
    "                'std_cost': std_cost,\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper,\n",
    "                'mean_gap': mean_gap,\n",
    "                'std_gap': std_gap,\n",
    "                'mean_time': mean_time,\n",
    "                'best_cost': group['final_cost'].min(),\n",
    "                'worst_cost': group['final_cost'].max()\n",
    "            }\n",
    "            stats_list.append(stats_dict)\n",
    "        \n",
    "        # Crear DataFrame con estad√≠sticas\n",
    "        param_stats = pd.DataFrame(stats_list)\n",
    "        param_stats = param_stats.sort_values('mean_cost')\n",
    "        \n",
    "        results[param] = param_stats\n",
    "        \n",
    "        # Mostrar tabla\n",
    "        display_cols = [param, 'n_runs', 'mean_cost', 'ci_lower', 'ci_upper', 'mean_gap', 'best_cost']\n",
    "        print(param_stats[display_cols].round(2).to_string(index=False))\n",
    "        \n",
    "        # Identificar el mejor valor\n",
    "        best_idx = param_stats['mean_cost'].idxmin()\n",
    "        best_value = param_stats.loc[best_idx, param]\n",
    "        best_cost = param_stats.loc[best_idx, 'mean_cost']\n",
    "        best_gap = param_stats.loc[best_idx, 'mean_gap']\n",
    "        \n",
    "        print(f\"\\nüèÜ MEJOR VALOR: {param} = {best_value}\")\n",
    "        print(f\"   - Costo promedio: {best_cost:.2f}\")\n",
    "        print(f\"   - Gap promedio: {best_gap:.2f}%\")\n",
    "        \n",
    "        # Test ANOVA para verificar diferencias significativas\n",
    "        groups = [group['final_cost'].values for _, group in grouped]\n",
    "        f_stat, p_value = stats.f_oneway(*groups)\n",
    "        \n",
    "        print(f\"\\nüßÆ TEST ANOVA:\")\n",
    "        print(f\"   - F-statistic: {f_stat:.3f}\")\n",
    "        print(f\"   - p-value: {p_value:.6f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(f\"   ‚úÖ Diferencias SIGNIFICATIVAS entre valores de {param}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå NO hay diferencias significativas entre valores de {param}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Ejecutar an√°lisis estad√≠stico\n",
    "if df is not None:\n",
    "    param_statistics = calculate_statistics_by_parameter(df)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No hay datos para el an√°lisis estad√≠stico.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d6e28",
   "metadata": {},
   "source": [
    "## 5. Visualize Parameter Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameter_visualizations(df):\n",
    "    \"\"\"\n",
    "    Crea visualizaciones comprehensivas para el an√°lisis de par√°metros\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        print(\"‚ö†Ô∏è  No hay datos para visualizar.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    fig.suptitle('An√°lisis de Par√°metros del Algoritmo ACS', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    parameters = ['colony_size', 'alpha', 'beta', 'q0']\n",
    "    \n",
    "    for i, param in enumerate(parameters):\n",
    "        ax = axes[i//2, i%2]\n",
    "        \n",
    "        # Box plot con intervalos de confianza\n",
    "        sns.boxplot(data=df, x=param, y='final_cost', ax=ax)\n",
    "        ax.set_title(f'Distribuci√≥n de Costos por {param}', fontweight='bold')\n",
    "        ax.set_ylabel('Costo Final')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rotar etiquetas si es necesario\n",
    "        if param in ['alpha', 'beta', 'q0']:\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Gr√°fico de barras con intervalos de confianza\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    fig.suptitle('Costo Promedio con Intervalos de Confianza (95%)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, param in enumerate(parameters):\n",
    "        ax = axes[i//2, i%2]\n",
    "        \n",
    "        # Calcular estad√≠sticas por par√°metro\n",
    "        stats_df = df.groupby(param).agg({\n",
    "            'final_cost': ['mean', 'std', 'count']\n",
    "        }).reset_index()\n",
    "        \n",
    "        stats_df.columns = [param, 'mean', 'std', 'count']\n",
    "        \n",
    "        # Calcular intervalos de confianza\n",
    "        stats_df['sem'] = stats_df['std'] / np.sqrt(stats_df['count'])\n",
    "        stats_df['ci'] = 1.96 * stats_df['sem']  # 95% CI\n",
    "        \n",
    "        # Gr√°fico de barras\n",
    "        bars = ax.bar(range(len(stats_df)), stats_df['mean'], \n",
    "                     yerr=stats_df['ci'], capsize=5, alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel(param)\n",
    "        ax.set_ylabel('Costo Promedio')\n",
    "        ax.set_title(f'Costo Promedio por {param}')\n",
    "        ax.set_xticks(range(len(stats_df)))\n",
    "        ax.set_xticklabels(stats_df[param], rotation=45 if param in ['alpha', 'beta', 'q0'] else 0)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Destacar el mejor valor\n",
    "        best_idx = stats_df['mean'].idxmin()\n",
    "        bars[best_idx].set_color('red')\n",
    "        bars[best_idx].set_alpha(0.9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_correlation_analysis(df):\n",
    "    \"\"\"\n",
    "    An√°lisis de correlaci√≥n entre par√°metros y rendimiento\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    print(\"üîó AN√ÅLISIS DE CORRELACI√ìN\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Matriz de correlaci√≥n\n",
    "    corr_columns = ['colony_size', 'alpha', 'beta', 'q0', 'final_cost', 'gap_percentage', 'execution_time']\n",
    "    corr_matrix = df[corr_columns].corr()\n",
    "    \n",
    "    # Visualizar matriz de correlaci√≥n\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdYlBu_r', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Matriz de Correlaci√≥n entre Par√°metros y M√©tricas de Rendimiento', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # An√°lisis de correlaci√≥n con el costo final\n",
    "    print(\"üéØ CORRELACI√ìN CON EL COSTO FINAL:\")\n",
    "    for param in ['colony_size', 'alpha', 'beta', 'q0']:\n",
    "        correlation, p_value = pearsonr(df[param], df['final_cost'])\n",
    "        print(f\"   - {param}: r = {correlation:.3f}, p = {p_value:.6f}\")\n",
    "        \n",
    "        if abs(correlation) > 0.3:\n",
    "            direction = \"positiva\" if correlation > 0 else \"negativa\"\n",
    "            strength = \"fuerte\" if abs(correlation) > 0.7 else \"moderada\"\n",
    "            print(f\"     ‚û§ Correlaci√≥n {direction} {strength}\")\n",
    "        else:\n",
    "            print(f\"     ‚û§ Correlaci√≥n d√©bil\")\n",
    "\n",
    "def create_performance_summary(df):\n",
    "    \"\"\"\n",
    "    Crea un resumen visual del rendimiento general\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # 1. Distribuci√≥n de costos finales\n",
    "    axes[0].hist(df['final_cost'], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[0].axvline(df['final_cost'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Media: {df[\"final_cost\"].mean():.2f}')\n",
    "    axes[0].axvline(df['final_cost'].median(), color='orange', linestyle='--', \n",
    "                   label=f'Mediana: {df[\"final_cost\"].median():.2f}')\n",
    "    axes[0].set_xlabel('Costo Final')\n",
    "    axes[0].set_ylabel('Frecuencia')\n",
    "    axes[0].set_title('Distribuci√≥n de Costos Finales')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Gap percentage vs execution time\n",
    "    scatter = axes[1].scatter(df['execution_time'], df['gap_percentage'], \n",
    "                             c=df['final_cost'], cmap='viridis', alpha=0.6)\n",
    "    axes[1].set_xlabel('Tiempo de Ejecuci√≥n (s)')\n",
    "    axes[1].set_ylabel('Gap Percentage (%)')\n",
    "    axes[1].set_title('Tiempo vs Calidad de Soluci√≥n')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[1], label='Costo Final')\n",
    "    \n",
    "    # 3. Box plot del gap percentage\n",
    "    axes[2].boxplot(df['gap_percentage'])\n",
    "    axes[2].set_ylabel('Gap Percentage (%)')\n",
    "    axes[2].set_title('Distribuci√≥n del Gap Percentage')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Resumen de Rendimiento del Algoritmo ACS', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Crear visualizaciones\n",
    "if df is not None:\n",
    "    print(\"üìä Generando visualizaciones...\")\n",
    "    create_parameter_visualizations(df)\n",
    "    create_correlation_analysis(df)\n",
    "    create_performance_summary(df)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No hay datos para crear visualizaciones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01845bda",
   "metadata": {},
   "source": [
    "## 6. Generate Performance Reports and Best Parameter Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac05e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best_parameters_report(df, param_statistics):\n",
    "    \"\"\"\n",
    "    Genera un reporte completo con las mejores configuraciones de par√°metros\n",
    "    \"\"\"\n",
    "    if df is None or param_statistics is None:\n",
    "        print(\"‚ö†Ô∏è  No hay datos suficientes para generar el reporte.\")\n",
    "        return\n",
    "    \n",
    "    print(\"üèÜ REPORTE DE MEJORES PAR√ÅMETROS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Mejores valores individuales por par√°metro\n",
    "    best_params = {}\n",
    "    \n",
    "    print(\"\\n1. üéØ MEJORES VALORES INDIVIDUALES:\")\n",
    "    for param in ['colony_size', 'alpha', 'beta', 'q0']:\n",
    "        stats = param_statistics[param]\n",
    "        best_idx = stats['mean_cost'].idxmin()\n",
    "        best_value = stats.loc[best_idx, param]\n",
    "        best_cost = stats.loc[best_idx, 'mean_cost']\n",
    "        best_gap = stats.loc[best_idx, 'mean_gap']\n",
    "        \n",
    "        best_params[param] = best_value\n",
    "        \n",
    "        print(f\"   ‚Ä¢ {param}: {best_value}\")\n",
    "        print(f\"     - Costo promedio: {best_cost:.2f}\")\n",
    "        print(f\"     - Gap promedio: {best_gap:.2f}%\")\n",
    "    \n",
    "    # 2. Mejor combinaci√≥n general\n",
    "    print(f\"\\n2. üåü MEJOR COMBINACI√ìN GENERAL:\")\n",
    "    \n",
    "    # Encontrar la mejor ejecuci√≥n individual\n",
    "    best_run_idx = df['final_cost'].idxmin()\n",
    "    best_run = df.loc[best_run_idx]\n",
    "    \n",
    "    print(f\"   ‚Ä¢ colony_size: {best_run['colony_size']}\")\n",
    "    print(f\"   ‚Ä¢ alpha: {best_run['alpha']}\")\n",
    "    print(f\"   ‚Ä¢ beta: {best_run['beta']}\")\n",
    "    print(f\"   ‚Ä¢ q0: {best_run['q0']}\")\n",
    "    print(f\"   ‚Ä¢ Costo obtenido: {best_run['final_cost']:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Gap: {best_run['gap_percentage']:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Tiempo: {best_run['execution_time']:.2f}s\")\n",
    "    \n",
    "    # 3. Top 5 configuraciones\n",
    "    print(f\"\\n3. ü•á TOP 5 MEJORES CONFIGURACIONES:\")\n",
    "    \n",
    "    # Agrupar por configuraci√≥n y obtener estad√≠sticas\n",
    "    config_stats = df.groupby(['colony_size', 'alpha', 'beta', 'q0']).agg({\n",
    "        'final_cost': ['mean', 'std', 'min'],\n",
    "        'gap_percentage': 'mean',\n",
    "        'execution_time': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    config_stats.columns = ['mean_cost', 'std_cost', 'best_cost', 'mean_gap', 'mean_time']\n",
    "    config_stats = config_stats.sort_values('mean_cost').head(5).reset_index()\n",
    "    \n",
    "    for i, row in config_stats.iterrows():\n",
    "        print(f\"   {i+1}. colony_size={row['colony_size']}, alpha={row['alpha']}, \"\n",
    "              f\"beta={row['beta']}, q0={row['q0']}\")\n",
    "        print(f\"      Costo: {row['mean_cost']:.2f} ¬± {row['std_cost']:.2f}, \"\n",
    "              f\"Gap: {row['mean_gap']:.2f}%, Tiempo: {row['mean_time']:.2f}s\")\n",
    "    \n",
    "    # 4. Recomendaciones basadas en an√°lisis\n",
    "    print(f\"\\n4. üí° RECOMENDACIONES:\")\n",
    "    \n",
    "    print(f\"   üéØ Para MEJOR CALIDAD DE SOLUCI√ìN:\")\n",
    "    best_quality_config = config_stats.iloc[0]\n",
    "    print(f\"   - Usar: colony_size={best_quality_config['colony_size']}, \"\n",
    "          f\"alpha={best_quality_config['alpha']}, beta={best_quality_config['beta']}, \"\n",
    "          f\"q0={best_quality_config['q0']}\")\n",
    "    \n",
    "    # Buscar configuraci√≥n con mejor balance calidad/tiempo\n",
    "    config_stats['efficiency'] = config_stats['mean_cost'] / config_stats['mean_time']\n",
    "    best_efficiency_idx = config_stats['efficiency'].idxmin()\n",
    "    best_efficiency_config = config_stats.iloc[best_efficiency_idx]\n",
    "    \n",
    "    print(f\"\\n   ‚ö° Para MEJOR BALANCE CALIDAD/TIEMPO:\")\n",
    "    print(f\"   - Usar: colony_size={best_efficiency_config['colony_size']}, \"\n",
    "          f\"alpha={best_efficiency_config['alpha']}, beta={best_efficiency_config['beta']}, \"\n",
    "          f\"q0={best_efficiency_config['q0']}\")\n",
    "    \n",
    "    # 5. Configuraci√≥n conservadora (valores seguros)\n",
    "    print(f\"\\n   üõ°Ô∏è CONFIGURACI√ìN CONSERVADORA (valores seguros):\")\n",
    "    print(f\"   - colony_size: {best_params['colony_size']} (mejor individual)\")\n",
    "    print(f\"   - alpha: {best_params['alpha']} (mejor individual)\")\n",
    "    print(f\"   - beta: {best_params['beta']} (mejor individual)\")\n",
    "    print(f\"   - q0: {best_params['q0']} (mejor individual)\")\n",
    "    \n",
    "    return {\n",
    "        'best_individual_params': best_params,\n",
    "        'best_overall_config': best_run[['colony_size', 'alpha', 'beta', 'q0']].to_dict(),\n",
    "        'top_5_configs': config_stats,\n",
    "        'best_quality_config': best_quality_config,\n",
    "        'best_efficiency_config': best_efficiency_config\n",
    "    }\n",
    "\n",
    "def save_analysis_report(df, recommendations, filename=None):\n",
    "    \"\"\"\n",
    "    Guarda un reporte completo del an√°lisis\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        filename = f\"acs_analysis_report_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"REPORTE DE AN√ÅLISIS DE PAR√ÅMETROS - ALGORITMO ACS\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Fecha: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total de experimentos: {len(df)}\\n\")\n",
    "        f.write(f\"Configuraciones √∫nicas: {len(df.groupby(['colony_size', 'alpha', 'beta', 'q0']))}\\n\\n\")\n",
    "        \n",
    "        # Estad√≠sticas generales\n",
    "        f.write(\"ESTAD√çSTICAS GENERALES:\\n\")\n",
    "        f.write(f\"- Mejor costo obtenido: {df['final_cost'].min():.2f}\\n\")\n",
    "        f.write(f\"- Costo promedio: {df['final_cost'].mean():.2f}\\n\")\n",
    "        f.write(f\"- Desviaci√≥n est√°ndar: {df['final_cost'].std():.2f}\\n\")\n",
    "        f.write(f\"- Mejor gap: {df['gap_percentage'].min():.2f}%\\n\")\n",
    "        f.write(f\"- Gap promedio: {df['gap_percentage'].mean():.2f}%\\n\\n\")\n",
    "        \n",
    "        # Mejores par√°metros\n",
    "        f.write(\"MEJORES CONFIGURACIONES:\\n\\n\")\n",
    "        f.write(\"1. Mejor configuraci√≥n general:\\n\")\n",
    "        best_config = recommendations['best_overall_config']\n",
    "        f.write(f\"   - colony_size: {best_config['colony_size']}\\n\")\n",
    "        f.write(f\"   - alpha: {best_config['alpha']}\\n\")\n",
    "        f.write(f\"   - beta: {best_config['beta']}\\n\")\n",
    "        f.write(f\"   - q0: {best_config['q0']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"2. Top 5 configuraciones:\\n\")\n",
    "        for i, row in recommendations['top_5_configs'].iterrows():\n",
    "            f.write(f\"   {i+1}. colony_size={row['colony_size']}, alpha={row['alpha']}, \"\n",
    "                   f\"beta={row['beta']}, q0={row['q0']} - Costo: {row['mean_cost']:.2f}\\n\")\n",
    "    \n",
    "    print(f\"üìÑ Reporte guardado en: {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Generar reporte final\n",
    "if df is not None and param_statistics is not None:\n",
    "    recommendations = generate_best_parameters_report(df, param_statistics)\n",
    "    report_file = save_analysis_report(df, recommendations)\n",
    "    \n",
    "    print(f\"\\n‚úÖ AN√ÅLISIS COMPLETADO\")\n",
    "    print(f\"üìä Datos analizados: {len(df)} experimentos\")\n",
    "    print(f\"üìÑ Reporte guardado: {report_file}\")\n",
    "    print(f\"üèÜ Mejores par√°metros identificados y validados estad√≠sticamente\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No se puede generar el reporte sin datos completos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde4a64",
   "metadata": {},
   "source": [
    "## 7. Conclusiones y Pr√≥ximos Pasos\n",
    "\n",
    "### Conclusiones del An√°lisis:\n",
    "\n",
    "1. **Validaci√≥n de Metodolog√≠a**: El notebook valida autom√°ticamente si tu metodolog√≠a de testing es estad√≠sticamente s√≥lida\n",
    "2. **An√°lisis Estad√≠stico Robusto**: Incluye intervalos de confianza, pruebas ANOVA y an√°lisis de correlaci√≥n\n",
    "3. **Visualizaciones Comprehensivas**: Gr√°ficos que permiten identificar patrones y tendencias f√°cilmente\n",
    "4. **Recomendaciones Basadas en Evidencia**: Identificaci√≥n de los mejores par√°metros con justificaci√≥n estad√≠stica\n",
    "\n",
    "### Para Mejorar tus Tests Actuales:\n",
    "\n",
    "1. **Ejecuta m√∫ltiples corridas** (m√≠nimo 5-10) por cada configuraci√≥n de par√°metros\n",
    "2. **Usa semillas diferentes** para cada corrida para garantizar independencia estad√≠stica\n",
    "3. **Almacena todos los resultados** en lugar de solo imprimirlos\n",
    "4. **Mide m√∫ltiples m√©tricas**: costo, tiempo de ejecuci√≥n, convergencia, etc.\n",
    "5. **Usa condiciones de parada consistentes** (solo por iteraciones, no por costo target)\n",
    "\n",
    "### Pr√≥ximos Pasos:\n",
    "\n",
    "1. Ejecuta el script `parameter_analysis_test.py` mejorado para generar datos\n",
    "2. Usa este notebook para analizar los resultados\n",
    "3. Considera an√°lisis adicionales como:\n",
    "   - An√°lisis de superficie de respuesta para interacciones entre par√°metros\n",
    "   - An√°lisis de convergencia temporal\n",
    "   - Comparaci√≥n con otros algoritmos metaheur√≠sticos"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
